[tool:pytest]
# Pytest configuration for beta diversity refactored package

# Test discovery
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*

# Test execution options
addopts = 
    --strict-markers
    --strict-config
    --verbose
    --tb=short
    --maxfail=5
    --durations=10
    --color=yes
    --disable-warnings

# Markers for test categorization
markers =
    unit: Unit tests for individual components
    integration: Integration tests for complete workflows
    performance: Performance and benchmark tests
    slow: Tests that take more than 5 seconds
    network: Tests that require network connectivity
    requires_data: Tests that require large test datasets
    parametrize: Parametrized tests with multiple inputs
    smoke: Quick smoke tests for basic functionality
    regression: Regression tests for bug fixes
    security: Security-related tests
    compatibility: Cross-platform/version compatibility tests

# Test coverage configuration
[coverage:run]
source = beta_diversity_refactored
omit = 
    */tests/*
    */test_*
    */__pycache__/*
    */.*
    setup.py
    run_tests.py

[coverage:report]
# Regexes for lines to exclude from consideration
exclude_lines =
    # Have to re-enable the standard pragma
    pragma: no cover

    # Don't complain about missing debug-only code:
    def __repr__
    if self\.debug

    # Don't complain if tests don't hit defensive assertion code:
    raise AssertionError
    raise NotImplementedError

    # Don't complain if non-runnable code isn't run:
    if 0:
    if __name__ == .__main__.:

    # Don't complain about abstract methods
    @(abc\.)?abstractmethod

ignore_errors = True
show_missing = True
precision = 1
skip_covered = False

[coverage:html]
directory = htmlcov
title = Beta Diversity Refactored - Coverage Report

[coverage:json]
output = coverage.json
pretty_print = true

# Benchmark configuration
[tool:pytest-benchmark]
min_rounds = 5
max_time = 5.0
min_time = 0.000005
sort = mean
histogram = true
save = .benchmarks
save_data = true
autosave = true
compare_fail = ['min:10%', 'max:20%', 'mean:10%', 'stddev:20%']
warmup = true
warmup_iterations = 10000

# Timeout configuration
timeout = 300
timeout_method = thread

# Parallel execution
[tool:pytest-xdist]
# Number of CPU cores to use (leave empty for auto-detection)
numprocesses = auto
# Distribution strategy
dist = worksteal

# Environment variables for testing
env = 
    TESTING = 1
    PYTEST_CURRENT_TEST = {nodeid}
    POLARS_MAX_THREADS = 2
    OMP_NUM_THREADS = 2

# Log configuration
log_cli = true
log_cli_level = INFO
log_cli_format = %(asctime)s [%(levelname)8s] %(name)s: %(message)s
log_cli_date_format = %Y-%m-%d %H:%M:%S

log_file = tests.log
log_file_level = DEBUG
log_file_format = %(asctime)s [%(levelname)8s] %(filename)s:%(lineno)d %(funcName)s(): %(message)s
log_file_date_format = %Y-%m-%d %H:%M:%S

# Filter warnings
filterwarnings =
    ignore::UserWarning
    ignore::DeprecationWarning
    ignore::PendingDeprecationWarning
    ignore::FutureWarning
    # Specific warnings to show
    always::pytest.PytestUnraisableExceptionWarning
    always::pytest.PytestConfigWarning
